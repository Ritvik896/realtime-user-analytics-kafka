## ðŸŽ¯ Project Overview

This project simulates a real-world SaaS platform tracking user interactions in real-time:

- **Producer**: Generates mock user events (clicks, purchases, logins, etc.)
- **Kafka**: Distributes events across topics with high throughput
- **Stream Processor**: Real-time aggregations and enrichment (Phase 3)
- **ML Pipeline**: Anomaly detection and churn prediction (Phase 5)
- **REST API**: Query analytics and insights (Phase 4)
- **Monitoring**: Prometheus + Grafana dashboards (Phase 6)

## ðŸš€ Quick Start (5 Minutes)

### Prerequisites
- Docker & Docker Compose
- Python 3.12  â† UPDATE FROM 3.10+
- Git

### Quick Start Commands

For fastest setup, copy and paste:
```bash
# 1. Create virtual environment
python3.12 -m venv venv
source venv/bin/activate

# 2. Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements/local.txt

# 3. Create monitoring config
mkdir -p monitoring
cat > monitoring/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
...
EOF

# 4. Start Docker
docker-compose up -d
sleep 10

# 5. Initialize database
python << 'EOF'
from src.database.connection import init_db, check_db_health
if check_db_health():
    init_db()
EOF

# 6. Run producer
python -m src.producer.user_event_producer --events 50 --rate 10

# 7. View events
# Open: http://localhost:8080
```

## ðŸ“Š Access Points

| Service | URL | Credentials | Status |
|---------|-----|-------------|--------|
| Kafka UI | http://localhost:8080 | None | Messages visible |
| Prometheus | http://localhost:9090 | None | Metrics (Phase 6+) |
| Grafana | http://localhost:3000 | admin/admin | Dashboards (Phase 6+) |
| PostgreSQL | localhost:5432 | analytics_user/analytics_pass | Phase 2+ |
| Kafka | localhost:9092 | - | Internal |

## ðŸ“š Project Structure

```
realtime-user-analytics-kafka/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ events/              # Event models and generator
â”‚   â”œâ”€â”€ producer/            # Kafka producer
â”‚   â”œâ”€â”€ consumer/            # Kafka consumer (Phase 2)
â”‚   â”œâ”€â”€ stream_processor/    # Stream processing (Phase 3)
â”‚   â”œâ”€â”€ ml/                  # ML models (Phase 5)
â”‚   â”œâ”€â”€ database/            # Database models (Phase 2)
â”‚   â”œâ”€â”€ api/                 # REST API (Phase 4)
â”‚   â””â”€â”€ utils/               # Utility functions
â”œâ”€â”€ config/                  # Configuration files
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ monitoring/              # Prometheus and Grafana configs
â”œâ”€â”€ requirements/            # Python dependencies
â””â”€â”€ docker-compose.yml       # Local development stack
```

## ðŸŽ“ Learning Phases

- **Phase 1**: Foundation (Event Models, Producer, Docker) âœ…
- **Phase 2**: Consumer (Data Storage, Database)
- **Phase 3**: Stream Processing (Real-time Aggregations)
- **Phase 4**: API & Analytics (REST Endpoints)
- **Phase 5**: ML Integration (Anomaly Detection)
- **Phase 6**: Monitoring (Prometheus + Grafana)
- **Phase 7**: Testing & CI/CD (Tests, GitHub Actions)

## ðŸ“– Documentation

- [LOCAL_SETUP.md](docs/LOCAL_SETUP.md) - Detailed setup guide
- [PROJECT_OVERVIEW.md](docs/PROJECT_OVERVIEW.md) - Project details
- [ARCHITECTURE.md](docs/ARCHITECTURE.md) - System architecture
- [CONTRIBUTING.md](CONTRIBUTING.md) - Git workflow

## ðŸ› ï¸ Common Commands

```bash
# Install
make install

# Setup everything
make dev

# Run producer
make producer

# Run tests
make test

# Format code
make format

# Lint code
make lint

# Docker
make docker-up
make docker-down
make docker-logs

# Help
make help
```

## ðŸ”§ Technology Stack

- **Streaming**: Apache Kafka
- **Processing**: Kafka Streams / Python
- **Database**: PostgreSQL
- **API**: FastAPI
- **ML**: scikit-learn
- **Monitoring**: Prometheus + Grafana
- **Containerization**: Docker & Docker Compose
- **Version Control**: Git & GitHub

## ðŸ“ˆ What You'll Learn

âœ… Apache Kafka (producers, consumers, topics)
âœ… Stream Processing (aggregations, joins)
âœ… Data Pipelines (ETL/ELT patterns)
âœ… Database Design (PostgreSQL, migrations)
âœ… REST APIs (FastAPI, validation)
âœ… Machine Learning (anomaly detection)
âœ… DevOps (Docker, monitoring, deployment)
âœ… Git Workflow (branches, PRs, collaboration)

## ðŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for development guidelines and Git workflow.

## ðŸ“ License

MIT License - See LICENSE file for details.

## ðŸš€ Next Steps

1. âœ… Complete setup
2. âœ… Run producer and see events in Kafka UI
3. âœ… Explore Kafka topics and messages
4. â†’ **Phase 2**: Build consumer to store events in PostgreSQL

## ðŸ“§ Questions?

Check the [docs/](docs/) folder for detailed guides and troubleshooting.

----------------------------------------------------------------------------------------------------------------------------------

# Phase 1: Completion & Branch Setup

## âœ… PHASE 1 STATUS

All your systems are working perfectly! âœ…

```
âœ… Producer: Sending 50 events (100% success rate)
âœ… Kafka: Receiving and storing events
âœ… Kafka UI: http://localhost:8080 (see messages)
âœ… PostgreSQL: Ready for storage
âœ… Docker: All 6 services healthy
âœ… Prometheus: Running (collecting metrics)
âœ… Grafana: Running (dashboard ready)
```

---

## 1ï¸âƒ£ PROMETHEUS & GRAFANA EMPTY - IS THIS NORMAL?

**YES, this is completely normal!** âœ…

Prometheus and Grafana are empty because:
- âœ… They just started
- âœ… Metrics take time to accumulate
- âœ… We haven't configured dashboards yet (Phase 6)
- âœ… They're running and healthy (which is what matters)

**This is expected in Phase 1** - Don't worry! They'll populate in later phases.

---

## 2ï¸âƒ£ YOUR PHASE 1 EXECUTION SUMMARY

### What You Accomplished

```
Phase 1: Foundation - Complete âœ…

âœ… Event Models
   - Created Pydantic models (UserEvent, PurchaseEvent, etc.)
   - Full validation & type safety
   - 5 specialized event types

âœ… Event Generator  
   - Mock data generator (no hardware needed)
   - 50 test users
   - Realistic user behavior patterns

âœ… Kafka Producer
   - Sends events to Kafka reliably
   - 50 events sent successfully
   - 100% success rate
   - 10.2 events/sec throughput

âœ… Docker Infrastructure
   - 6 services running (Kafka, Zookeeper, PostgreSQL, Prometheus, Grafana, Kafka UI)
   - All services healthy
   - Connection pooling configured

âœ… Code Quality
   - Professional structure
   - Comprehensive logging
   - Error handling
   - Configuration management

âœ… Monitoring Stack
   - Prometheus metrics collection
   - Grafana dashboards (waiting for Phase 6)
   - Health checks on all services
```

### Metrics Achieved

| Metric | Value |
|--------|-------|
| Events Generated | 50 |
| Success Rate | 100.0% |
| Throughput | 10.2 events/sec |
| Data Sent | 19,497 bytes |
| Services Healthy | 6/6 |
| Errors | 0 |

---

## 3ï¸âƒ£ PHASE 1 LEARNING DOCUMENTATION

Create new file: `docs/PHASE_1_LEARNING.md`

```markdown
# Phase 1: Foundation - Learning Summary

## What is Phase 1?

Phase 1 is about building the **event generation and message queue infrastructure**. It's the foundation that all other phases depend on.

## Key Concepts Learned

### 1. Event Streaming Architecture
- **Producer**: Generates mock user events
- **Message Queue (Kafka)**: Distributes events reliably
- **Consumer** (coming in Phase 2): Reads and processes events

### 2. Data Modeling with Pydantic
- Type safety through validation
- Automatic serialization
- Schema enforcement

### 3. Kafka Fundamentals
- Topics: Named event streams
- Partitions: Parallel processing
- Consumer groups: Scalable consumption
- Offset tracking: Replay capability

### 4. Containerization with Docker
- Service isolation
- Environment consistency
- Easy scaling
- Health monitoring

### 5. Monitoring & Observability
- Prometheus: Metrics collection
- Grafana: Visualization
- Health checks: Service reliability

## What You Built

1. **Event Models** (`src/events/event_models.py`)
   - UserEvent base class
   - Specialized event types (Purchase, Video, Click, Search)
   - Full validation

2. **Event Generator** (`src/events/event_generator.py`)
   - Mock realistic user behavior
   - 50 test users
   - Session simulation

3. **Kafka Producer** (`src/producer/user_event_producer.py`)
   - Sends events to Kafka
   - Error handling & retries
   - Metrics tracking

4. **Docker Setup** (`docker-compose.yml`)
   - 6 integrated services
   - Health checks
   - Network isolation

## Key Metrics

- 50 events successfully produced
- 100% reliability rate
- 10.2 events per second throughput
- 0 errors

## How It Works

```
User Behavior (Mocked)
      â†“
Event Generator
      â†“
Event Model (Validated)
      â†“
Kafka Producer
      â†“
Kafka Topic (Distributed)
      â†“
Visible in Kafka UI: http://localhost:8080
```

## What's Next

**Phase 2**: Build consumer to read events and store in PostgreSQL

## Hands-On Skills Developed

âœ… Pydantic for data validation
âœ… Apache Kafka basics
âœ… Docker containerization
âœ… Event-driven architecture
âœ… Async programming concepts
âœ… Logging & monitoring
âœ… Professional code structure

## Common Issues & Solutions

### Issue: "kafka_vendor.six.moves not found"
**Solution**: Use confluent-kafka instead of kafka-python

### Issue: "Connection refused"
**Solution**: Ensure docker-compose up -d is running

### Issue: "Prometheus showing empty"
**Solution**: Normal! Metrics accumulate over time. Dashboards come in Phase 6.

## Success Criteria (All Met âœ…)

- âœ… Events generated successfully
- âœ… Events sent to Kafka
- âœ… Kafka UI shows messages
- âœ… All Docker services healthy
- âœ… No errors in production
- âœ… Code follows professional standards

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Event Generator (Mocked)   â”‚
â”‚  - 50 test users            â”‚
â”‚  - Realistic behavior       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Producer             â”‚
â”‚  - Validates events         â”‚
â”‚  - Sends to Kafka           â”‚
â”‚  - Tracks metrics           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topic: user-events   â”‚
â”‚  - Stores events            â”‚
â”‚  - Distributes to consumers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka UI (Visualization)   â”‚
â”‚  - http://localhost:8080    â”‚
â”‚  - View all messages        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Database Design (Coming Phase 2)

Events will be stored with:
- event_id (unique)
- user_id (trackable)
- timestamp (queryable)
- event_type (filterable)
- metadata (flexible)

## Performance Baseline

From Phase 1 execution:
- Throughput: 10.2 events/sec
- Success Rate: 100%
- Latency: < 100ms per event
- Reliability: 0 errors

This baseline will improve with optimizations in later phases.

## Key Takeaways

1. **Event-driven architecture** is about producers and consumers
2. **Kafka** is a reliable message broker for streaming
3. **Pydantic** provides type safety and validation
4. **Docker** ensures consistency across environments
5. **Monitoring** is essential from day one
6. **Professional code** scales better and is easier to debug

## Next Steps

Ready for Phase 2! ðŸš€

Phase 2 will:
- Build a Kafka consumer
- Create PostgreSQL database
- Store events permanently
- Track user statistics
- Handle errors gracefully
```

---

## 4ï¸âƒ£ GIT BRANCH SETUP & COMMIT

### Step 1: Check Current Status

```bash
# Check current branch
git branch
# Shows: * master (or main)

# Check what's changed
git status
# Shows: modified and untracked files
```

### Step 2: Create Phase 1 Feature Branch

```bash
# Create and switch to feature branch
git checkout -b feature/phase-1-foundation

# Verify you're on new branch
git branch
# Shows: * feature/phase-1-foundation
```

### Step 3: Stage All Changes

```bash
# Add all Phase 1 files and changes
git add .

# Verify what will be committed
git status
# Should show all files ready to commit
```

### Step 4: Commit Phase 1 Changes

```bash
git commit -m "feat(phase-1): Complete foundation with event generation and Kafka producer

Phase 1 Implementation:
- Event Models: Pydantic models for all event types (UserEvent, Purchase, Video, Click, Search)
- Event Generator: Mock user behavior simulator with 50 test users
- Kafka Producer: Reliable event sender with error handling
- Docker Setup: Complete infrastructure (Kafka, Zookeeper, PostgreSQL, Prometheus, Grafana)
- Configuration: Environment setup and constants management

Features Implemented:
- Full event validation with Pydantic
- Realistic user behavior patterns
- 100% message delivery reliability
- Connection pooling for performance
- Structured logging
- Health checks on all services

Testing Results:
- 50 events successfully produced
- 100% success rate
- 10.2 events/sec throughput
- 0 errors
- All 6 Docker services healthy

Architecture:
- Event generation â†’ Kafka producer â†’ Kafka topic â†’ Kafka UI
- Monitoring stack ready (Prometheus, Grafana)
- Database ready for Phase 2

Next Phase: Consumer and Database Storage"
```

### Step 5: Push to GitHub

```bash
# Push feature branch
git push -u origin feature/phase-1-foundation

# Verify on GitHub
# Go to: https://github.com/YOUR_USERNAME/realtime-user-analytics-kafka
# You should see feature/phase-1-foundation branch
```

### Step 6: Create Pull Request (On GitHub)

1. Go to your GitHub repository
2. Click **Pull requests** tab
3. Click **New pull request**
4. Select:
   - **Base**: main (or master)
   - **Compare**: feature/phase-1-foundation
5. Click **Create pull request**
6. Add title: "Phase 1: Event Generation & Kafka Producer"
7. Copy commit message as description
8. Click **Create pull request**

### Step 7: Merge to Main

```bash
# On GitHub PR page, click "Merge pull request"
# Confirm merge

# Locally:
git checkout main  # or master
git pull origin main

# Verify Phase 1 is on main
git log --oneline -2
# Should show Phase 1 commit
```

### Step 8: Delete Feature Branch

```bash
# Delete locally
git branch -d feature/phase-1-foundation

# Delete on GitHub
git push origin --delete feature/phase-1-foundation
```

---

## 5ï¸âƒ£ FINAL VERIFICATION CHECKLIST

Before starting Phase 2:

```bash
# âœ… All services running
docker-compose ps
# All should show: Up

# âœ… Producer works
python -m src.producer.user_event_producer --events 10 --rate 10
# Should show: success_rate: 100.0%

# âœ… Kafka UI accessible
curl http://localhost:8080/api/clusters
# Should return JSON

# âœ… Code changes committed
git log --oneline -1
# Should show Phase 1 commit

# âœ… Branch merged
git branch
# Should show: * main (or master)
```

---

## ðŸ“Š PHASE 1 SUMMARY

| Item | Status |
|------|--------|
| Event Models | âœ… Complete |
| Event Generator | âœ… Complete |
| Kafka Producer | âœ… Complete |
| Docker Setup | âœ… Complete |
| Testing | âœ… Pass (100%) |
| Documentation | âœ… Complete |
| Git Branch | âœ… Ready to merge |
| Learning Doc | âœ… Created |

---

## ðŸš€ READY FOR PHASE 2

Once you complete the Git steps above, you're ready to start Phase 2!

Phase 2 will add:
- Kafka Consumer
- PostgreSQL Storage
- User Statistics Tracking
- Error Handling with Dead-Letter Queue