"""
Template for environment variables.
Copy this to .env and fill in your values.
SAFE TO COMMIT (no secrets included).
DO NOT commit .env to git (add to .gitignore)
"""

# ============================================================
# KAFKA CONFIGURATION
# ============================================================
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_EVENTS=user-events
KAFKA_TOPIC_ANALYTICS=user-analytics
KAFKA_TOPIC_ANOMALIES=user-anomalies
KAFKA_CONSUMER_GROUP=user-analytics-group

# ============================================================
# DATABASE CONFIGURATION
# ============================================================
# Local Development
DB_HOST=localhost
DB_PORT=5432
DB_NAME=user_analytics
DB_USER=analytics_user
DB_PASSWORD=analytics_pass

# Connection String (for SQLAlchemy)
DATABASE_URL=postgresql://analytics_user:analytics_pass@localhost:5432/user_analytics

# Connection Pooling Settings (Phase 2)
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_RECYCLE=3600
DB_POOL_TIMEOUT=30

# AWS RDS (For future production)
# DATABASE_URL=postgresql://user:password@mydb.xxxxx.us-east-1.rds.amazonaws.com:5432/user_analytics

# ============================================================
# API CONFIGURATION
# ============================================================
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=True

# ============================================================
# LOGGING
# ============================================================
LOG_LEVEL=INFO
LOG_FORMAT=json

# ============================================================
# PRODUCER CONFIGURATION
# ============================================================
PRODUCER_BATCH_SIZE=100
PRODUCER_RATE_PER_SECOND=10

# ============================================================
# CONSUMER CONFIGURATION (Phase 2)
# ============================================================
CONSUMER_BATCH_SIZE=100
CONSUMER_TIMEOUT_MS=10000
CONSUMER_GROUP_ID=user-analytics-group
CONSUMER_AUTO_OFFSET_RESET=earliest
CONSUMER_AUTO_COMMIT_ENABLED=false
CONSUMER_MAX_POLL_RECORDS=500
CONSUMER_STATS_INTERVAL=30
CONSUMER_ENABLE_DLQ=true

# ============================================================
# ML CONFIGURATION (Phase 5)
# ============================================================
ML_ANOMALY_THRESHOLD=0.7
ML_MODEL_UPDATE_INTERVAL=3600

# ============================================================
# AWS CONFIGURATION
# ============================================================
AWS_REGION=us-east-1
AWS_ENVIRONMENT=local

# For AWS Deployment (uncomment and fill when deploying):
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# 
# AWS MSK (Managed Streaming for Kafka)
# KAFKA_BOOTSTRAP_SERVERS=b-1.msk.xxxxx.kafka.us-east-1.amazonaws.com:9092,b-2.msk.xxxxx.kafka.us-east-1.amazonaws.com:9092
#
# AWS RDS (PostgreSQL)
# DATABASE_URL=postgresql://username:password@mydb.xxxxx.us-east-1.rds.amazonaws.com:5432/user_analytics